# 02일차 - 텍스트 데이터 분석 및 시각화

> **🎯 오늘의 목표**: 텍스트 데이터를 분석하고, Wordcloud와 텍스트 마이닝 기법으로 인사이트를 발견합니다!

```table-of-contents
```

---

## 📚 이 날 다루는 핵심 기술

> **📝 참고**: 이 강의는 오소진 대표님께서 진행하시며, 3일차부터는 1-2일차 내용을 기반으로 심화 학습을 진행합니다.

### 🐍 Python 라이브러리

**텍스트 전처리**:
- `re`: 정규표현식으로 텍스트 정제
- `collections.Counter`: 단어 빈도 계산

**한글 텍스트 처리**:
- `konlpy.tag.Okt`: 한글 형태소 분석기
  - `morphs()`: 형태소 분석
  - `nouns()`: 명사 추출
  - `pos()`: 품사 태깅

**텍스트 시각화**:
- `wordcloud.WordCloud`: 워드클라우드 생성
- `matplotlib`: 텍스트 시각화 결과 표시

**데이터 분석**:
- `pandas`: 텍스트 데이터 관리 및 집계 (1일차 내용 활용)

### 💡 핵심 개념

**텍스트 전처리**:
```python
re.sub(r'[^\w\s]', '', text)    # 특수문자 제거
text.lower()                     # 소문자 변환
text.split()                     # 토큰화 (공백 기준)
Counter(words)                   # 단어 빈도 계산
```

**한글 형태소 분석**:
```python
from konlpy.tag import Okt
okt = Okt()

okt.morphs(text)                # 형태소 분석
okt.nouns(text)                 # 명사만 추출
okt.pos(text)                   # 품사 태깅
```

**워드클라우드**:
```python
WordCloud(font_path='...', ...)  # 워드클라우드 객체 생성
.generate(text)                  # 텍스트로 생성
.generate_from_frequencies(dict) # 빈도 딕셔너리로 생성
```

**불용어 처리**:
```python
stopwords = ['조사', '접속사', ...]
filtered = [w for w in words if w not in stopwords]
```

### 🎯 학습 목표

1. ✅ 정규표현식으로 텍스트 정제 및 토큰화
2. ✅ konlpy로 한글 형태소 분석 및 명사 추출
3. ✅ 단어 빈도 계산 및 Top N 추출
4. ✅ WordCloud로 텍스트 시각화
5. ✅ pandas와 텍스트 분석 통합
6. ✅ 실전 프로젝트: 대통령 연설문 특징 비교

---

## 📋 Overview

**오늘 배울 내용**:
1. 텍스트 데이터 전처리 기초
   - 불용어 제거, 형태소 분석, 토큰화
2. NETFLIX 영화 데이터 분석
   - 영화 제목, 장르, 설명 분석
   - Wordcloud 시각화
3. 대통령 연설문 텍스트 마이닝
   - 한글 텍스트 형태소 분석
   - 연설문 특징 추출
4. 팀 프로젝트
   - 각 대통령의 연설문 특징 비교 분석

---

## 📦 필요한 라이브러리 설치

+ `pandas`, `numpy` : 데이터 분석
+ `konlpy` : 텍스트 처리
+ `matplotlib`, `seaborn`, `wordcloud` : 시각화

**한글 형태소 분석기 설치** (Mac/Linux):
```bash
# Java 설치 (konlpy 의존성)
# Mac: brew install openjdk@11

# konlpy 설치
pip install konlpy
```

**Windows**:
```bash
# JPype1 먼저 설치 (konlpy 의존성)
pip install JPype1

# konlpy 설치
pip install konlpy
```

---

## 📝 Section 1. 텍스트 데이터 전처리 기초

### 1-1. 텍스트 정제 및 토큰화

**개념**:
- **토큰화(Tokenization)**: 문장을 단어 단위로 분리
- **불용어(Stopwords)**: 분석에 불필요한 단어 (조사, 접속사 등)
- **정규화(Normalization)**: 대소문자 통일, 특수문자 제거

**필요한 문법**:
```python
import re
from collections import Counter

# 기본 텍스트 전처리
text = "안녕하세요! 오늘은 날씨가 좋네요. 데이터 분석을 시작해봅시다."

# 특수문자 제거
text_clean = re.sub(r'[^\w\s]', '', text)
print(text_clean)

# 단어 분리 (공백 기준)
words = text_clean.split()
print(words)

# 단어 빈도 계산
word_counts = Counter(words)
print(word_counts.most_common(5))
```

**실습 . 영어 텍스트 전처리**:
```python
import re
from collections import Counter

text = """
Data science is an interdisciplinary field that uses scientific methods,
processes, algorithms and systems to extract knowledge and insights from
structured and unstructured data.
"""

# 소문자 변환
text_lower = text.lower()

# 특수문자 및 숫자 제거
text_clean = re.sub(r'[^a-zA-Z\s]', '', text_lower)

# 토큰화
words = text_clean.split()

# 불용어 제거 (간단한 예시)
stopwords = ['is', 'an', 'that', 'to', 'and', 'from']
words_filtered = [word for word in words if word not in stopwords]

# 단어 빈도 계산
word_counts = Counter(words_filtered)

print("=== Top 10 단어 ===")
for word, count in word_counts.most_common(10):
    print(f"{word}: {count}")
```

### 1-2. 한글 형태소 분석

**개념**:
- **형태소(Morpheme)**: 의미를 가진 가장 작은 단위
- **품사 태깅(POS Tagging)**: 각 단어의 품사 분류

**실습 . konlpy를 사용한 형태소 분석**:
```python
from konlpy.tag import Okt
from collections import Counter

okt = Okt()

text = "대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다."

# 형태소 분석
morphs = okt.morphs(text)
print("=== 형태소 ===")
print(morphs)

# 명사만 추출
nouns = okt.nouns(text)
print("\n=== 명사 ===")
print(nouns)

# 품사 태깅
pos = okt.pos(text)
print("\n=== 품사 태깅 ===")
for word, tag in pos[:10]:
    print(f"{word}/{tag}")

# 명사 빈도 계산
noun_counts = Counter(nouns)
print("\n=== 명사 빈도 ===")
for noun, count in noun_counts.most_common(5):
    print(f"{noun}: {count}")
```

---

## 🎬 Section 2. NETFLIX 영화 데이터 분석

### 📂 데이터 출처

**Netflix 영화/TV 프로그램 데이터**:
1. **Kaggle - Netflix Movies and TV Shows** (Kaggle 계정 필요)
   - 🔗 https://www.kaggle.com/datasets/shivamb/netflix-shows
   - 가장 인기있는 Netflix 데이터셋 (16,000+ 다운로드)
   - 파일형식: CSV
   - 포함정보: 제목, 장르, 설명, 개봉년도, 국가, 평점 등

2. **Kaggle - Netflix Movies and TV shows till 2025** (최신 데이터)
   - 🔗 https://www.kaggle.com/datasets/bhargavchirumamilla/netflix-movies-and-tv-shows-till-2025
   - 2025년 3월까지 업데이트된 최신 데이터
   - 파일형식: CSV

3. **Kaggle - Netflix TV Shows and Movies** (by Victor Soeiro)
   - 🔗 https://www.kaggle.com/datasets/victorsoeiro/netflix-tv-shows-and-movies
   - 5,000+ 고유 타이틀, 15개 컬럼
   - 2022년 7월 데이터

**데이터 다운로드 방법**:
1. Kaggle 계정 생성 (무료): https://www.kaggle.com/
2. 원하는 데이터셋 페이지 접속
3. 우측 상단 "Download" 버튼 클릭
4. CSV 파일을 프로젝트 폴더에 저장

**💡 팁**:
- Kaggle은 데이터 과학자들이 가장 많이 사용하는 데이터셋 플랫폼입니다
- 실습에서는 샘플 데이터를 사용하지만, 실제 프로젝트에서는 Kaggle 데이터를 활용하세요!
- 한글 Netflix 데이터가 필요하면 국내 OTT 플랫폼 데이터를 직접 수집해야 합니다

---

### 2-1. 영화 데이터 탐색

**실습 . NETFLIX 영화 데이터 분석**:
```python
import pandas as pd
from collections import Counter

# 샘플 NETFLIX 영화 데이터
data = {
    '제목': ['기생충', '오징어 게임', '킹덤', '스위트홈', '승리호', '서복', '지옥', '종이의 집'],
    '장르': ['드라마, 스릴러', '드라마, 액션', '공포, 드라마', '공포, 액션', 'SF, 액션', 'SF, 드라마', '공포, 드라마', '액션, 드라마'],
    '설명': [
        '빈부 격차를 다룬 블랙 코미디',
        '생존 게임 서바이벌 드라마',
        '조선시대 좀비 스릴러',
        '괴물과의 생존 공포',
        '우주 쓰레기 청소부들의 모험',
        '복제인간을 다룬 SF 액션',
        '지옥 사자가 나타나는 종말론적 드라마',
        '스페인 은행 강도 시리즈'
    ],
    '개봉년도': [2019, 2021, 2019, 2020, 2021, 2021, 2021, 2017]
}

df_netflix = pd.DataFrame(data)

# 데이터 확인
print("=== NETFLIX 영화 데이터 ===")
print(df_netflix)

# 장르 분석
all_genres = []
for genres in df_netflix['장르']:
    all_genres.extend([g.strip() for g in genres.split(',')])

genre_counts = Counter(all_genres)

print("\n=== 장르 분포 ===")
for genre, count in genre_counts.most_common():
    print(f"{genre}: {count}")
```

**실습 . 영화 설명 텍스트 분석**:
```python
from konlpy.tag import Okt
from collections import Counter

okt = Okt()

# 모든 설명 텍스트 합치기
all_descriptions = ' '.join(df_netflix['설명'])

# 명사 추출
nouns = okt.nouns(all_descriptions)

# 한 글자 단어 제거
nouns_filtered = [noun for noun in nouns if len(noun) > 1]

# 빈도 계산
noun_counts = Counter(nouns_filtered)

print("=== 영화 설명에서 자주 나오는 단어 (Top 10) ===")
for word, count in noun_counts.most_common(10):
    print(f"{word}: {count}")
```

### 2-2. Wordcloud 시각화

**개념**:
- 단어 빈도에 비례하여 크기를 다르게 표시
- 시각적으로 텍스트 데이터의 주요 키워드 파악

**실습 . Wordcloud 생성**:
```python
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from konlpy.tag import Okt
from collections import Counter

okt = Okt()

# 한글 폰트 설정
plt.rcParams['font.family'] = 'AppleGothic'  # Mac
# plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows

# 영화 설명 텍스트
all_descriptions = ' '.join(df_netflix['설명'])

# 명사 추출
nouns = okt.nouns(all_descriptions)
nouns_filtered = [noun for noun in nouns if len(noun) > 1]

# 텍스트로 합치기
text_for_wordcloud = ' '.join(nouns_filtered)

# Wordcloud 생성
wordcloud = WordCloud(
    font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',  # Mac 한글 폰트
    # font_path='c:/Windows/Fonts/malgun.ttf',  # Windows 한글 폰트
    width=800,
    height=400,
    background_color='white'
).generate(text_for_wordcloud)

# 시각화
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('NETFLIX 영화 설명 Wordcloud')
plt.tight_layout()
plt.show()
```

**실습 . 빈도 기반 Wordcloud (더 정확한 방법)**:
```python
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# 단어 빈도 계산
noun_counts = Counter(nouns_filtered)

# 빈도 딕셔너리로 Wordcloud 생성
wordcloud = WordCloud(
    font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',
    width=800,
    height=400,
    background_color='white',
    colormap='viridis'  # 색상 테마
).generate_from_frequencies(noun_counts)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('NETFLIX 영화 주요 키워드 (빈도 기반)')
plt.tight_layout()
plt.show()
```

---

## 🎤 Section 3. 대통령 연설문 텍스트 마이닝

### 📂 데이터 출처

**대통령 연설문 데이터**:
1. **대통령기록관 (행정안전부)** (로그인 없이 다운로드 가능)
   - 🔗 https://www.pa.go.kr/
   - 메뉴: 대통령기록 → 연설기록
   - 제공범위: 1대 이승만 ~ 19대 문재인 대통령
   - 포함정보: 연설 전문, 음성 파일, 날짜, 연설 유형

2. **공공데이터포털** (로그인 없이 파일 다운로드 가능)
   - 🔗 https://www.data.go.kr/data/15084168/fileData.do
   - 데이터명: "행정안전부 대통령기록관_대통령연설기록(음성)"
   - 179건의 주요 연설 음성 + 전문
   - 파일형식: CSV, JSON

3. **현직 대통령 (윤석열 대통령)**
   - 🔗 https://www.president.go.kr/ko/open_data.php
   - 메뉴: 정책/정보 → 열린 데이터
   - 실시간 업데이트되는 최신 연설문

**연설문 유형**:
- 신년사, 취임사, 국회 시정연설
- 광복절/3.1절 등 기념사
- 국정연설, 대국민 담화
- 기자회견, 환영사

**데이터 검색 기능**:
- 대통령별, 연설 유형별, 연도별 검색 가능
- 키워드 검색 지원

**💡 팁**:
- 대통령기록관은 역대 대통령의 모든 공식 기록을 보관합니다
- 실습에서는 샘플 연설문을 사용하지만, 실제 분석 시 위 사이트에서 전문을 다운로드하세요!
- 최신 연설문은 청와대/대통령실 홈페이지에서 제공됩니다

---

### 3-1. 연설문 데이터 준비

**실습 . 연설문 데이터 생성**:
```python
import pandas as pd

# 샘플 대통령 연설문 데이터
speeches = {
    '대통령': ['문재인', '문재인', '윤석열', '윤석열', '박근혜', '박근혜'],
    '제목': [
        '2022년 신년사', '2021년 광복절 경축사',
        '2023년 신년사', '2022년 취임사',
        '2017년 신년 기자회견', '2016년 국회 시정연설'
    ],
    '내용': [
        '국민 여러분, 새해 복 많이 받으십시오. 2022년 새해를 맞아 국민께 인사드립니다. 지난해 우리는 코로나19 위기 속에서도 경제를 지키고 일자리를 만들었습니다.',
        '존경하는 국민 여러분, 광복 76주년을 맞이했습니다. 우리는 일제 강점기를 극복하고 자유와 민주주의를 지켜냈습니다.',
        '존경하는 국민 여러분, 2023년 새해가 밝았습니다. 올해는 자유와 번영의 새로운 시대를 열어가는 한 해가 되어야 합니다.',
        '존경하는 국민 여러분, 오늘 저는 대한민국 제20대 대통령으로서 역사적 소명을 다하기 위해 이 자리에 섰습니다.',
        '국민 여러분, 2017년 새해를 맞이하여 국민께 인사드립니다. 우리 경제가 어려운 시기를 겪고 있지만 함께 극복해 나가겠습니다.',
        '존경하는 국회의장과 국회의원 여러분, 오늘 국회 시정연설을 통해 국정 운영 방향을 말씀드립니다.'
    ]
}

df_speeches = pd.DataFrame(speeches)

print("=== 대통령 연설문 데이터 ===")
print(df_speeches['대통령', '제목'](%27%EB%8C%80%ED%86%B5%EB%A0%B9%27%2C%20%27%EC%A0%9C%EB%AA%A9%27.md))
```

### 3-2. 연설문 텍스트 분석

**실습 . 대통령별 주요 키워드 분석**:
```python
from konlpy.tag import Okt
from collections import Counter
import matplotlib.pyplot as plt

okt = Okt()

# 대통령별로 그룹화
presidents = df_speeches.groupby('대통령')['내용'].apply(lambda x: ' '.join(x)).to_dict()

# 대통령별 명사 추출
president_keywords = {}

for president, text in presidents.items():
    # 명사 추출
    nouns = okt.nouns(text)

    # 필터링: 2글자 이상, 불용어 제거
    stopwords = ['저', '것', '수', '등', '안', '더', '때', '위해']
    nouns_filtered = [noun for noun in nouns if len(noun) > 1 and noun not in stopwords]

    # 빈도 계산
    noun_counts = Counter(nouns_filtered)
    president_keywords[president] = noun_counts.most_common(10)

# 결과 출력
for president, keywords in president_keywords.items():
    print(f"\n=== {president} 대통령 주요 키워드 ===")
    for word, count in keywords:
        print(f"{word}: {count}")
```

**실습 . 대통령별 Wordcloud 비교**:
```python
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 한글 폰트 설정
plt.rcParams['font.family'] = 'AppleGothic'

# 서브플롯으로 여러 Wordcloud 표시
fig, axes = plt.subplots(1, len(presidents), figsize=(18, 5))

for idx, (president, text) in enumerate(presidents.items()):
    # 명사 추출
    nouns = okt.nouns(text)
    stopwords = ['저', '것', '수', '등', '안', '더', '때', '위해']
    nouns_filtered = [noun for noun in nouns if len(noun) > 1 and noun not in stopwords]

    # Wordcloud 생성
    text_for_wordcloud = ' '.join(nouns_filtered)

    wordcloud = WordCloud(
        font_path='/System/Library/Fonts/AppleSDGothicNeo.ttc',
        width=600,
        height=400,
        background_color='white'
    ).generate(text_for_wordcloud)

    # 서브플롯에 표시
    axes[idx].imshow(wordcloud, interpolation='bilinear')
    axes[idx].axis('off')
    axes[idx].set_title(f'{president} 대통령', fontsize=14)

plt.tight_layout()
plt.show()
```

### 3-3. 단어 빈도 시각화

**실습 . 막대그래프로 키워드 비교**:
```python
import matplotlib.pyplot as plt
import numpy as np

# 대통령별 Top 5 키워드
fig, axes = plt.subplots(1, len(president_keywords), figsize=(15, 5))

for idx, (president, keywords) in enumerate(president_keywords.items()):
    words = [word for word, count in keywords[:5]]
    counts = [count for word, count in keywords[:5]]

    axes[idx].barh(words, counts, color='skyblue')
    axes[idx].set_title(f'{president} 대통령')
    axes[idx].set_xlabel('빈도')
    axes[idx].invert_yaxis()

plt.tight_layout()
plt.show()
```

---

## 👥 팀 프로젝트: 각 대통령의 연설문 특징 분석

**프로젝트 목표**:
- 여러 대통령의 연설문을 분석하여 특징 비교
- 텍스트 마이닝 기법을 활용한 인사이트 도출

**요구사항**:
1. 대통령별 주요 키워드 Top 20 추출
2. Wordcloud로 시각화
3. 연설문 길이, 문장 수, 평균 단어 수 분석
4. 대통령별 언어 스타일 차이 분석
5. 발표 자료 작성 및 프레젠테이션

**프로젝트 템플릿**:
```python
import pandas as pd
from konlpy.tag import Okt
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

okt = Okt()

# 1. 데이터 로드
df = pd.read_csv('presidential_speeches.csv')

# 2. 대통령별 그룹화
# TODO: 대통령별로 연설문 그룹화

# 3. 텍스트 분석
# TODO: 명사 추출, 빈도 계산

# 4. 통계 분석
# TODO: 연설문 길이, 문장 수, 평균 단어 수

# 5. Wordcloud 생성
# TODO: 대통령별 Wordcloud 비교

# 6. 인사이트 도출
# TODO: 대통령별 언어 스타일, 주요 관심사 분석
```

**분석 포인트**:
- **주요 키워드**: 가장 자주 사용된 단어 Top 20
- **언어 스타일**: 문장 길이, 어휘 다양성, 복잡도
- **주제 분석**: 경제, 외교, 복지 등 주제별 빈도
- **감성 분석**: 긍정/부정 단어 사용 비율
- **시대별 비교**: 같은 대통령의 시기별 변화

**평가 기준**:
- 데이터 분석의 깊이와 정확성
- 시각화의 명확성과 창의성
- 인사이트의 논리성과 설득력
- 팀워크 및 발표 완성도

---

## 💡 핵심 정리

### 텍스트 전처리
```python
import re
from collections import Counter

# 특수문자 제거
text_clean = re.sub(r'[^\w\s]', '', text)

# 소문자 변환
text_lower = text.lower()

# 토큰화
words = text.split()

# 불용어 제거
words_filtered = [w for w in words if w not in stopwords]

# 빈도 계산
word_counts = Counter(words_filtered)
```

### 한글 형태소 분석
```python
from konlpy.tag import Okt

okt = Okt()

# 형태소 분석
morphs = okt.morphs(text)

# 명사 추출
nouns = okt.nouns(text)

# 품사 태깅
pos = okt.pos(text)
```

### Wordcloud 생성
```python
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# 텍스트로 생성
wordcloud = WordCloud(
    font_path='한글_폰트_경로',
    width=800,
    height=400,
    background_color='white'
).generate(text)

# 빈도로 생성 (더 정확)
wordcloud = WordCloud(...).generate_from_frequencies(word_counts)

# 시각화
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

### 빈도 분석
```python
from collections import Counter

# 단어 빈도
word_counts = Counter(words)

# Top N
top_words = word_counts.most_common(N)

# 특정 단어 빈도
count = word_counts['단어']
```

---

## AI 활용 프롬프트 모음

**텍스트 전처리**:
```
"정규표현식으로 이메일 주소만 추출하는 방법을 알려줘"
"한글 텍스트에서 특수문자와 영어를 제거하는 방법을 알려줘"
"pandas에서 텍스트 데이터를 전처리하는 다양한 방법을 알려줘"
```

**형태소 분석**:
```
"konlpy의 여러 형태소 분석기 차이점을 알려줘"
"한글 텍스트에서 동사만 추출하는 방법을 알려줘"
"품사 태깅 결과를 DataFrame으로 정리하는 방법을 알려줘"
```

**Wordcloud**:
```
"Wordcloud에 마스크 이미지를 적용하는 방법을 알려줘"
"Wordcloud 색상을 커스터마이징하는 방법을 알려줘"
"특정 단어를 Wordcloud에서 제외하는 방법을 알려줘"
```

**고급 분석**:
```
"TF-IDF를 사용한 문서 간 유사도 계산 방법을 알려줘"
"한글 감성 분석을 수행하는 방법을 알려줘"
"토픽 모델링(LDA)으로 주제를 추출하는 방법을 알려줘"
```

**시각화**:
```
"텍스트 데이터를 네트워크 그래프로 시각화하는 방법을 알려줘"
"단어 빈도를 트리맵으로 표현하는 방법을 알려줘"
"시간에 따른 키워드 변화를 라인차트로 그리는 방법을 알려줘"
```

---

## 🎓 2일차 학습 완료!

**오늘 배운 것**:
1. ✅ 텍스트 데이터 전처리 (정규표현식, 토큰화)
2. ✅ konlpy를 사용한 한글 형태소 분석
3. ✅ NETFLIX 영화 데이터 텍스트 분석
4. ✅ Wordcloud를 사용한 텍스트 시각화
5. ✅ 대통령 연설문 텍스트 마이닝
6. ✅ 팀 프로젝트: 연설문 특징 비교 분석

**다음 학습**:
- pandas 심화 기법
- 데이터 시각화 (matplotlib, seaborn)
- Excel 대시보드 자동화
- Streamlit 웹 대시보드 구축

**학습 팁**:
- 텍스트 분석은 전처리가 핵심! 데이터를 깨끗하게 정제하세요.
- 형태소 분석기마다 특성이 다르니 여러 분석기를 비교해보세요.
- Wordcloud는 인사이트를 빠르게 파악하는 좋은 도구입니다.
- 팀 프로젝트에서는 다양한 각도로 데이터를 분석해보세요!
